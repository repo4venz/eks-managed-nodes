apiVersion: v1
kind: Pod
metadata:
  name: gpu-workload-cuda
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: node.kubernetes.io/instance-type  # Use cloud-specific
            operator: In
            values: [p3.2xlarge, p2.xlarge, g4dn.xlarge, g5.xlarge, g5.2xlarge]  # AWS types
			
  containers:
  - name: cuda-container
    image: nvidia/cuda:12.2.0-base-ubuntu22.04
    command: ["/bin/bash", "-c", "--"]
    args: ["while true; do sleep 30; done;"]
    resources:
      limits:
        nvidia.com/gpu: 1  # Request 1 GPU
    securityContext:
      capabilities:
        add: ["SYS_ADMIN"]  # Often needed for GPU workloads

  # Optional: Tolerations for tainted nodes
  tolerations:
  - key: "nvidia.com/gpu"
    operator: "Exists"
    effect: "NoSchedule"

